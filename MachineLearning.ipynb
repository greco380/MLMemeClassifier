#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Real Imports
from __future__ import absolute_import, division, print_function, unicode_literals


import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import os
import numpy as np
import matplotlib.pyplot as plt

from skimage import data, color
from skimage.transform import rescale, resize, downscale_local_mean

import cv2

from datetime import datetime

import keras
from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dropout, BatchNormalization

from keras.models import Sequential
from keras.layers import *
# from keras.optimizers import SGD
import keras.utils
from keras import utils as np_utils
keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)
# from keras.layers import Dense, Dropout, Activation
# from keras.optimizers import SGD



PATH = os.path.join(os.path.dirname('/users/u27/greco/memes/AllData'), 'AllData')

# parse through the folder through every file


# In[4]:


train_dir = os.path.join(PATH, 'Train/')
val_dir = os.path.join(PATH, 'Validate/')

train_blb_dir = os.path.join(train_dir, 'TrainBadLBri')  # directory with training set of BadLuckBrian
train_dbf_dir = os.path.join(train_dir, 'TrainDisBf')  # directory with training set of DisloyalBoyfriend
train_htp_dir = os.path.join(train_dir, 'TrainHideTPain')  # directory with training set of HideThePain
train_sb_dir = os.path.join(train_dir, 'TrainSponge')  # directory with training set of Spongebob
train_tb_dir = os.path.join(train_dir, 'TrainTwoBt')  # directory with training set of TwoButtons

val_blb_dir = os.path.join(val_dir, 'ValBadLBri')  # directory with validating set of BadLuckBrian
val_dbf_dir = os.path.join(val_dir, 'ValDisBf')  # directory with validating set of DisloyalBoyfriend
val_htp_dir = os.path.join(val_dir, 'ValHideTPain')  # directory with validating set of HideThePain
val_sb_dir = os.path.join(val_dir, 'ValSponge')  # directory with validating set of Spongebob
val_tb_dir = os.path.join(val_dir, 'ValTwoBt')  # directory with validating set of TwoButtons

num_blb_tr = len(os.listdir(train_blb_dir))
num_dbf_tr = len(os.listdir(train_dbf_dir))
num_htp_tr = len(os.listdir(train_htp_dir))
num_sb_tr = len(os.listdir(train_sb_dir))
num_tb_tr = len(os.listdir(train_tb_dir))

num_blb_val = len(os.listdir(val_blb_dir))
num_dbf_val = len(os.listdir(val_dbf_dir))
num_htp_val = len(os.listdir(val_htp_dir))
num_sb_val = len(os.listdir(val_sb_dir))
num_tb_val = len(os.listdir(val_tb_dir))

total_train = num_blb_tr + num_dbf_tr + num_htp_tr + num_sb_tr + num_tb_tr
total_val = num_blb_val + num_dbf_val + num_htp_val + num_sb_val + num_tb_val


# In[17]:


print("BLB training set: ", num_blb_tr)
print("DBF training set: ", num_dbf_tr)
print("HTP training set: ", num_htp_tr)
print("SB training set: ", num_sb_tr)
print("TB training set: ", num_tb_tr)

print("\n\n")

print("BLB Validation set: ", num_blb_val)
print("DBF Validation set: ", num_dbf_val)
print("HTP Validation set: ", num_htp_val)
print("SB Validation set: ", num_sb_val)
print("TB Validation set: ", num_tb_val)

print("\n\n")

print("Total Training: ", total_train)
print("Total Validation: ", total_val)


# In[18]:


batch_size = 128
epochs = 1
IMG_HEIGHT = 150
IMG_WIDTH = 150


# In[19]:


train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data


# In[20]:


train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                           directory=train_dir,
                                                           shuffle=True,
                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                           class_mode='categorical')


# In[7]:


val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                              directory=val_dir,
                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                              class_mode='categorical')


# In[21]:


sample_training_images, _ = next(train_data_gen)


# In[22]:


def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()


# In[23]:


plotImages(sample_training_images[:5])


# In[24]:


model = Sequential([
    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(5, activation= 'softmax')
])


# In[12]:


model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])


# In[13]:


history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)


# In[25]:


model.summary()


# In[45]:


# !pip install -q -I pyyaml h5py --user

model.save("my_meme_model.h5")


# In[1]:


# model.predict("/users/u27/greco/memes/use/test20200214-223043-535063.png")


CATEGORIES = ["Bad Luck Brian", "Distracted boyfriend", "Hide the Pain", "Spongebob", "Two Buttons"]

new_filepath = "/users/u27/greco/testData/test20200214-223043-535063.png"

def prepare(filepath):
    IMG_SIZE = 150
    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)

model = tf.keras.models.load_model("my_meme_model.h5")

prediction = model.predict([prepare(new_filepath)])

print("It should predict Two Buttons: \n")
print(CATEGORIES[int(prediction[0][0][0][0][0])


# In[ ]:


history = model.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)


# In[ ]:


image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)


# In[45]:


train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))


# In[46]:


augmented_images = [train_data_gen[0][0][0] for i in range(5)]


# In[47]:


# Re-use the same custom plotting function defined and used
# above to visualize the training images
plotImages(augmented_images)


# In[48]:


image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)


# In[49]:


train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))

augmented_images = [train_data_gen[0][0][0] for i in range(5)]


# In[50]:


plotImages(augmented_images)


# In[51]:


# zoom_range from 0 - 1 where 1 = 100%.
image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5) #


# In[52]:


train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,
                                               directory=train_dir,
                                               shuffle=True,
                                               target_size=(IMG_HEIGHT, IMG_WIDTH))

augmented_images = [train_data_gen[0][0][0] for i in range(5)]


# In[53]:


plotImages(augmented_images)


# In[54]:


image_gen_train = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=45,
                    width_shift_range=.15,
                    height_shift_range=.15,
                    horizontal_flip=True,
                    zoom_range=0.5
                    )


# In[55]:


train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,
                                                     directory=train_dir,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                     class_mode='categorical')


# In[56]:


augmented_images = [train_data_gen[0][0][0] for i in range(5)]
plotImages(augmented_images)


# In[57]:


image_gen_val = ImageDataGenerator(rescale=1./255)


# In[58]:


val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,
                                                 directory=val_dir,
                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                 class_mode='categorical') # categorical_crossentropy


# In[59]:


model_new = Sequential([
    Conv2D(16, 3, padding='same', activation='relu',
           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
    MaxPooling2D(),
    Dropout(0.2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPooling2D(),
    Dropout(0.2),
    Flatten(),
    Dense(512, activation='relu'), #change last one to softmax
    Dense(1)
])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


# In[60]:


model_new.compile(optimizer='SGD',
                  loss=tf.keras.losses.categorical_crossentropy(from_logits=True),
                  metrics=['accuracy'])
# model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy', 'categorical_accuracy'])
# model.compile(loss=customLoss, optimizer='SGD', metrics=['accuracy', 'categorical_accuracy'])


model_new.summary()


# In[ ]:


history = model_new.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)


# In[ ]:


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


# In[ ]:


# print(model(r'/home/greco/Documents/Memev2/PersonalTestsResize/20200209-070152-411881.jpeg'))

get_ipython().system('pip install -q pyyaml h5py --user')

model.save('my_meme_model.h5')


# In[ ]:


# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('my_meme_model.h5')

# Show the model architecture
new_model.summary()


# In[ ]:


history = model_new.fit_generator(
    train_data_gen,
    steps_per_epoch=total_train // batch_size,
    epochs=epochs,
    validation_data=val_data_gen,
    validation_steps=total_val // batch_size
)


# In[ ]:


acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


# In[ ]:


# print(model(r'/home/greco/Documents/Memev2/PersonalTestsResize/20200209-070152-411881.jpeg'))

get_ipython().system('pip install -q pyyaml h5py --user')

model.save('my_meme_model.h5')


# In[ ]:


# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('my_meme_model.h5')

# Show the model architecture
new_model.summary()


# In[ ]:


# use the trained model and plug in a test image
#output: what meme it is

# answer = mz

# prediction = my_meme_model.h5.predict(__file__)


# In[ ]:



# from PIL import Image

# myImage = Image.open(r'/home/greco/Documents/Memev2/PersonalTestsResize/20200209-070152-411881.jpeg')

# myImage.show()


# In[ ]:





# In[ ]:


# train_image_generator = ImageDataGenerator(rescale=1./255)


# In[ ]:


# train_dir = '/home/greco/Documents/Memev2/PersonalTestsResize/'

# train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
#                                                            directory=train_dir,
#                                                            shuffle=True,
#                                                            target_size=(IMG_HEIGHT, IMG_WIDTH),
#                                                            class_mode='binary')


# In[ ]:


# !pip install tensorflow_hub --user
# import tensorflow_hub as hub
# # detector = hub.load(model_new)


# In[ ]:


# !pip install opencv-python --user

# #!pip install cv
# import cv2

# CATEGORIES = ["SpongeBob", "Change My Mind"]

# imgg = cv2.imread('path.tif', 1)

# model_new.summary()

# def format_example(myImage, label):
#   myImage = tf.cast(image, tf.float32)
#   image = (myImage/255) - 1
#   image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
#   return image, label
# print(format_example)

# # def prepare(filepath):
# #     IMG_SIZE = 70  # 50 in txt-based
# # #     imgg = cv2.imread('path.tif', 1)
# #     img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)

# prediction = model.predict(train_dir)
# print(prediction)  # will be a list in a list.
# print(CATEGORIES[int(prediction[0][0])])


# In[ ]:


# predictionFinal = model_new.predict(myImage)

# print(predictionFinal)


# In[ ]:



